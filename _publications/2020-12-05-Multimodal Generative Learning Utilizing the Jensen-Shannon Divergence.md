---
title: "Multimodal Generative Learning Utilizing the Jensen-Shannon Divergence"
collection: publications
permalink: /publication/2020-12-05-Multimodal Generative Learning Utilizing the Jensen-Shannon Divergence
excerpt: 'Learning from different data types is a long-standing goal in machine learning research, as multiple information sources co-occur when describing natural phenomena. However, existing generative models that approximate a multimodal ELBO rely on difficult or inefficient training schemes to learn a joint distribution and the dependencies between modalities. In this work, we propose a novel, efficient objective function that utilizes the Jensen-Shannon divergence for multiple distributions. It simultaneously approximates the unimodal and joint multimodal posteriors directly via a dynamic prior. In addition, we theoretically prove that the new multimodal JS-divergence (mmJSD) objective optimizes an ELBO. In extensive experiments, we demonstrate the advantage of the proposed mmJSD model compared to previous work in unsupervised, generative learning tasks.'
date: 2020-12-05
venue: 'Neurips 2020'
paperurl: 'https://proceedings.neurips.cc/paper/2020/hash/43bb733c1b62a5e374c63cb22fa457b4-Abstract.html'
citation: 'TM Sutter, I Daunhawer, JE Vogt. (2020). &quot;Multimodal Generative Learning Utilizing the Jensen-Shannon Divergence.&quot; <i>Neurips 2020</i>. 1(1).'
---
Learning from different data types is a long-standing goal in machine learning research, as multiple information sources co-occur when describing natural phenomena. However, existing generative models that approximate a multimodal ELBO rely on difficult or inefficient training schemes to learn a joint distribution and the dependencies between modalities. In this work, we propose a novel, efficient objective function that utilizes the Jensen-Shannon divergence for multiple distributions. It simultaneously approximates the unimodal and joint multimodal posteriors directly via a dynamic prior. In addition, we theoretically prove that the new multimodal JS-divergence (mmJSD) objective optimizes an ELBO. In extensive experiments, we demonstrate the advantage of the proposed mmJSD model compared to previous work in unsupervised, generative learning tasks.

[Download paper here](https://proceedings.neurips.cc/paper/2020/hash/43bb733c1b62a5e374c63cb22fa457b4-Abstract.html)

Recommended citation: TM Sutter, I Daunhawer, JE Vogt. (2020). "Multimodal Generative Learning Utilizing the Jensen-Shannon Divergence." <i>Neurips 2020</i>. 1(1).